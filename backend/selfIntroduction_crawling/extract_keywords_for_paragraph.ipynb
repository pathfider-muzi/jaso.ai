{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from math import log\n",
    "from tqdm import tqdm\n",
    "from kss import split_sentences\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ElectraTokenizer.from_pretrained('brainelectra-base-discriminator')\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_data_dir = '../valid_data_question_seperated_paragraph/'\n",
    "data_list = os.listdir(paragraph_data_dir)\n",
    "tf_vocab = dict()\n",
    "tf_table = dict()\n",
    "paragraph_keywords = []\n",
    "word_book = dict()\n",
    "valid_id = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(t, d):\n",
    "\treturn d.count(t)\n",
    "\n",
    "def idf(df):\n",
    "\tN = len(df) + 1\n",
    "\tres = []\n",
    "\tcnt = 0\n",
    "\tfor v in df:\n",
    "\t\tfor i in df[v]:\n",
    "\t\t\tcnt += i > 0\n",
    "\t\tres.append(log(N / (cnt + 1)))\n",
    "\t\tcnt = 0\n",
    "\treturn res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_words(input_ids):\n",
    "\twords = []\n",
    "\tfor vectorized_text in input_ids['input_ids']:\n",
    "\t\tfor d in vectorized_text:\n",
    "\t\t\tword = tokenizer.decode(d)\n",
    "\t\t\tif word == '[ C L S ]' or word == '[ S E P ]': continue\n",
    "\t\t\tif word.find('#') < 0 and len(word) > 1:\n",
    "\t\t\t\twords.append(word)\n",
    "\treturn words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(content: str) -> list:\n",
    "\tinput_ids = tokenizer(content, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "\treturn _extract_words(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tf_vocab():\n",
    "\tfor d in tqdm(data_list, desc='TF_VOCAB_LISTING'):\n",
    "\t\tfile_name = paragraph_data_dir + d\n",
    "\t\twith open(file_name) as file:\n",
    "\t\t\tjson_file = json.load(file)\n",
    "\t\t\tid = json_file['id']\n",
    "\t\t\tif id not in tf_vocab.keys():\n",
    "\t\t\t\ttf_vocab[id] = []\n",
    "\t\t\ttf_vocab[id].extend(extract_words(content=json_file['answer']))\n",
    "\tfor k in tqdm(tf_vocab.keys(), desc='TF_VOCAB_UINQUE'):\n",
    "\t\ttf_vocab[k] = set(tf_vocab[k])\n",
    "\t\ttf_vocab[k] = list(tf_vocab[k])\n",
    "\twith open('tf_vocab.json', 'w') as f:\n",
    "\t\tjson.dump(tf_vocab, f, ensure_ascii=False, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF_VOCAB_LISTING: 100%|██████████| 31268/31268 [01:07<00:00, 463.17it/s]\n",
      "TF_VOCAB_UINQUE: 100%|██████████| 6597/6597 [00:00<00:00, 42011.31it/s]\n"
     ]
    }
   ],
   "source": [
    "make_tf_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF: 100%|██████████| 31268/31268 [01:15<00:00, 412.36it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(data_list, desc='TF'):\n",
    "\tfile_name = paragraph_data_dir + d\n",
    "\twith open(file_name) as file:\n",
    "\t\tjson_file = json.load(file)\n",
    "\t\tid = json_file['id']\n",
    "\t\tif id not in tf_table.keys():\n",
    "\t\t\ttf_table[id] = []\n",
    "\t\tdocs = extract_words(content=json_file['answer'])\n",
    "\n",
    "\t\t# TF\n",
    "\t\ttf_table[id].append([])\n",
    "\t\tfor t in tf_vocab[id]:\n",
    "\t\t\ttf_table[id][-1].append(tf(t, docs))\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF: 100%|██████████| 31268/31268 [37:54<00:00, 13.75it/s] \n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(data_list, desc='TF-IDF'):\n",
    "\tkeyword_json = dict()\n",
    "\tfile_name = paragraph_data_dir + d\n",
    "\twith open(file_name) as file:\n",
    "\t\tjson_file = json.load(file)\n",
    "\t\tid = json_file['id']\n",
    "\t\tkeyword_json = json_file\n",
    "\t\tkeyword_json['keywords'] = []\n",
    "\t\t\n",
    "\t\tif id not in valid_id:\n",
    "\t\t\tcur = 0\n",
    "\t\t\tvalid_id.append(id)\n",
    "\t\t\t# IDF\n",
    "\t\t\ttfidf = pd.DataFrame(tf_table[id], columns=tf_vocab[id])\n",
    "\t\t\tidf_ = pd.DataFrame(idf(tfidf), index=tf_vocab[id], columns=[\"IDF\"])\n",
    "\t\t\t# TF-IDF\n",
    "\t\t\tfor c in tfidf:\n",
    "\t\t\t\tfor i in range(len(tfidf)):\n",
    "\t\t\t\t\ttfidf[c] *= float(idf_.loc[c] + 1)\n",
    "\n",
    "\t\tmax_val = tfidf.loc[cur].max()\n",
    "\t\tfor c in tfidf:\n",
    "\t\t\tif tfidf.loc[cur][c] == max_val:\n",
    "\t\t\t\tkeyword_json['keywords'].append(c)\n",
    "\t\tparagraph_keywords.append(keyword_json)\n",
    "\tcur += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tf-idf_keyword_paragraph.json', 'w') as f:\n",
    "\tjson.dump(paragraph_keywords, f, ensure_ascii=False, indent=4, sort_keys=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5db0bf8e65298cb01341d1d0401e9d0f1ebbad7939f704a64363986abd13b2a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
